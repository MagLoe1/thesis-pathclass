{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script was used to clean the data (e.g. removing duplicates) and to add additional information extracted from the data (e.g. text statistics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from tqdm.notebook import tqdm\n",
    "# from ydata_profiling import ProfileReport\n",
    "from string import punctuation\n",
    "from global_variables import RANDOM_SEED, LANG\n",
    "import re\n",
    "\n",
    "# local files\n",
    "from preprocessing import Vocabulary # just for manual inspection of the remaining words\n",
    "from functions_small_helper import create_folder_if_not_exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPLACEMENT_TOKS = [\"[padnr]\", \"[namn]\", \"[pnr]\", \"[datum]\", \"[decimal]\", \"[time]\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<..\\241028_run_dataprep> exists:  True\n"
     ]
    }
   ],
   "source": [
    "# input data\n",
    "data_folder = os.path.join(\"..\", \"data\", \"data_raw\")\n",
    "data_filename = \"data_raw_replaced_labs.csv\"\n",
    "data_file = os.path.join(data_folder, data_filename)\n",
    "\n",
    "\n",
    "# output data\n",
    "clean_data_folder = os.path.join(\"..\", \"data\", \"data_clean\")\n",
    "create_folder_if_not_exists(clean_data_folder)\n",
    "\n",
    "data_cleaned = os.path.join(clean_data_folder, \"data_clean_no_dup\")\n",
    "data_partial_duplicates = os.path.join(clean_data_folder, \"data_clean_part_dup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>orig_index</th>\n",
       "      <th>lab</th>\n",
       "      <th>remark</th>\n",
       "      <th>date</th>\n",
       "      <th>icdo3code</th>\n",
       "      <th>icdo3text</th>\n",
       "      <th>snomed3code</th>\n",
       "      <th>snomed3text</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [orig_index, lab, remark, date, icdo3code, icdo3text, snomed3code, snomed3text, text]\n",
       "Index: []"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw = pd.read_csv(data_file, header=0, index_col=0, dtype=object, encoding=\"utf-8\")\n",
    "# add row to keep original index if needed for manual inspection\n",
    "if not \"orig_index\" in df_raw.columns:\n",
    "    df_raw.insert(loc=0, column=\"orig_index\", value=list(df_raw.index))\n",
    "df_raw.reset_index(inplace=True, drop=True)\n",
    "\n",
    "df_raw.head(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34051, 9)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 34051 entries, 0 to 34050\n",
      "Data columns (total 9 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   orig_index   34051 non-null  int64 \n",
      " 1   lab          34051 non-null  object\n",
      " 2   remark       24002 non-null  object\n",
      " 3   date         31077 non-null  object\n",
      " 4   icdo3code    34051 non-null  object\n",
      " 5   icdo3text    34051 non-null  object\n",
      " 6   snomed3code  34039 non-null  object\n",
      " 7   snomed3text  34039 non-null  object\n",
      " 8   text         34051 non-null  object\n",
      "dtypes: int64(1), object(8)\n",
      "memory usage: 2.3+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df_raw.shape)\n",
    "print(df_raw.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Remove full duplicates and rows where snomed (=morphology, or morphology+grade) codes are missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicated rows:  472\n",
      "Original shape: (34051, 9) \n",
      "--> New shape: (33579, 9)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# create new df that will be manipulated to remove full duplicates\n",
    "df = df_raw.copy(deep=True)\n",
    "\n",
    "print(f\"Number of duplicated rows: \", df.iloc[:, 1:].duplicated().sum())    # exclude \"orig_index\" column since it will not be a duplicate\n",
    "df = df[df.iloc[:, 1:].duplicated()==False]\n",
    "assert df.iloc[:, 1:].duplicated().sum() == 0\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "print(f\"Original shape: {df_raw.shape} \\n--> New shape: {df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values:\n",
      "Removed 12 reports without snomed code.\n",
      " New shape:  (33567, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>orig_index</th>\n",
       "      <th>lab</th>\n",
       "      <th>remark</th>\n",
       "      <th>date</th>\n",
       "      <th>icdo3code</th>\n",
       "      <th>icdo3text</th>\n",
       "      <th>snomed3code</th>\n",
       "      <th>snomed3text</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [orig_index, lab, remark, date, icdo3code, icdo3text, snomed3code, snomed3text, text]\n",
       "Index: []"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Number of missing values:\")\n",
    "no_snomed_index = df[df[\"snomed3code\"].isna()].index\n",
    "df = df.drop(no_snomed_index,  inplace=False)\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "print(f\"Removed {len(no_snomed_index)} reports without snomed code.\\n\",\n",
    "      \"New shape: \", df.shape)\n",
    "df.tail(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Replace missing dates and add year column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>orig_index</th>\n",
       "      <th>lab</th>\n",
       "      <th>remark</th>\n",
       "      <th>date</th>\n",
       "      <th>icdo3code</th>\n",
       "      <th>icdo3text</th>\n",
       "      <th>snomed3code</th>\n",
       "      <th>snomed3text</th>\n",
       "      <th>text</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [orig_index, lab, remark, date, icdo3code, icdo3text, snomed3code, snomed3text, text, year]\n",
       "Index: []"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# replace date\n",
    "df[\"date\"] = df[\"date\"].fillna(\"1900-01-01\")\n",
    "\n",
    "# add year column for eaiser access\n",
    "def get_year(datestring):\n",
    "    year, month, day = datestring.split(\"-\")\n",
    "    return int(year)\n",
    "\n",
    "df[\"year\"] = df[\"date\"].apply(get_year)\n",
    "df.tail(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Add site and subsite columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>orig_index</th>\n",
       "      <th>lab</th>\n",
       "      <th>remark</th>\n",
       "      <th>date</th>\n",
       "      <th>icdo3code</th>\n",
       "      <th>icdo3text</th>\n",
       "      <th>snomed3code</th>\n",
       "      <th>snomed3text</th>\n",
       "      <th>text</th>\n",
       "      <th>year</th>\n",
       "      <th>site</th>\n",
       "      <th>subsite</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [orig_index, lab, remark, date, icdo3code, icdo3text, snomed3code, snomed3text, text, year, site, subsite]\n",
       "Index: []"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_site(topo_code):\n",
    "    assert len(topo_code) == 4  # C509\n",
    "    return int(topo_code[1:-1]) # 50\n",
    "\n",
    "def get_subsite(topo_code):\n",
    "    assert len(topo_code) == 4\n",
    "    return int(topo_code[-1])   # 9\n",
    "\n",
    "df[\"site\"] = df[\"icdo3code\"].apply(get_site)\n",
    "df[\"subsite\"] = df[\"icdo3code\"].apply(get_subsite)\n",
    "df.sample(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Add site+subsite column (for accessing site directly in multitask prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>orig_index</th>\n",
       "      <th>lab</th>\n",
       "      <th>remark</th>\n",
       "      <th>date</th>\n",
       "      <th>icdo3code</th>\n",
       "      <th>icdo3text</th>\n",
       "      <th>snomed3code</th>\n",
       "      <th>snomed3text</th>\n",
       "      <th>text</th>\n",
       "      <th>year</th>\n",
       "      <th>site</th>\n",
       "      <th>subsite</th>\n",
       "      <th>site+subsite</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [orig_index, lab, remark, date, icdo3code, icdo3text, snomed3code, snomed3text, text, year, site, subsite, site+subsite]\n",
       "Index: []"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_site_subsite(topo_code):\n",
    "    assert len(topo_code) == 4\n",
    "    return int(topo_code[1:])\n",
    "\n",
    "df[\"site+subsite\"] = df[\"icdo3code\"].apply(get_site_subsite)\n",
    "df.head(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the code into subparts 12345(6) -> morphology: 1234, behavior: 5, grade: 6 (though only for few reports)\n",
    "def get_hist(snomend_code):\n",
    "    return snomend_code[0:4]\n",
    "\n",
    "def get_behavior(snomed_code):\n",
    "    if len(snomed_code) == 5:\n",
    "        return snomed_code[-1]\n",
    "    elif len(snomed_code) == 6:\n",
    "        return snomed_code[-2]\n",
    "\n",
    "def get_grade(snomed_code):\n",
    "    if len(snomed_code) == 6:\n",
    "        return snomed_code[-1]\n",
    "    else:\n",
    "        return \"None\"\n",
    "\n",
    "\n",
    "def get_morph_no_grade(morph_code):\n",
    "    return morph_code[:5]\n",
    "\n",
    "df[\"histology\"] = df[\"snomed3code\"].apply(get_hist)\n",
    "df[\"behavior\"] = df[\"snomed3code\"].apply(get_behavior)\n",
    "df[\"morphology\"] = df[\"snomed3code\"].apply(get_morph_no_grade)\n",
    "df[\"grade\"] = df[\"snomed3code\"].apply(get_grade)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of different full morphology codes (all (5 or 6) digits): 140\n",
      "Number of different morphology codes (only 5 digit, no grade): 131\n",
      "Number of different histology codes (first 4 digits): 99\n",
      "Number of different behavior codes (5th digit): 4 ['3' '2' '1' '0']\n",
      "Number of different grade codes (6th digit): 5 ['None' '2' '3' '1' '6']\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of different full morphology codes (all (5 or 6) digits):\", len(df[\"snomed3code\"].unique()))\n",
    "print(\"Number of different morphology codes (only 5 digit, no grade):\", len(df[\"morphology\"].unique()))\n",
    "print(\"Number of different histology codes (first 4 digits):\", len(df[\"histology\"].unique()))\n",
    "print(\"Number of different behavior codes (5th digit):\", len(df[\"behavior\"].unique()), df[\"behavior\"].unique())\n",
    "print(\"Number of different grade codes (6th digit):\", len(df[\"grade\"].unique()), df[\"grade\"].unique())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) Analyse text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# many texts have the same few words in the beginning, e.g \"macroscopie : [padnr] [namn], [namn]+ [pnr]\"\n",
    "# one lab also has a date in the beginning; both of these are removed, to avoid classification based on these patterns\n",
    "\n",
    "def remove_prefix(text):\n",
    "    # remove all combinations of the replacement tokens [padnr], [namn], [pnr], and the punctuation marks , : and whitespace\n",
    "    text = re.sub(pattern=r\"macroscopie : ((\\[padnr\\])|(\\[pnr\\])|(\\[namn\\])|[,: \\-])+\", repl=\"\", string=text, count=1)\n",
    "\n",
    "    if text.startswith(\"dikt datum\"):\n",
    "        text = re.sub(pattern=r\"dikt datum (20)?\\d\\d - \\d\\d - \\d\\d \\. \", repl=\"\", string=text, count=1)\n",
    "    return text\n",
    "\n",
    "# minimize vocabulary, similar to previous papers\n",
    "def replace_date(text):\n",
    "    # for dates that ocurr in the text;\n",
    "    text = re.sub(pattern=r\"(20)?\\d\\d - \\d\\d - \\d\\d\", repl=\"[datum]\", string=text, count=0) # count=0, all will be replaced\n",
    "    return text\n",
    "def replace_decimal(text):\n",
    "    text = re.sub(pattern=r\"(\\d , \\d cm)\", repl=\"[decimal] cm\", string=text, count=0) # count=0, all will be replaced\n",
    "    text = re.sub(pattern=r\"(\\d , \\d mm)\", repl=\"[decimal] mm\", string=text, count=0)\n",
    "    return text\n",
    "\n",
    "def replace_large_numbers(text):\n",
    "    text = re.sub(pattern=r\"\\d\\d\\d+\", repl=\"[large_number]\", string=text, count=0)\n",
    "    return text\n",
    "\n",
    "def replace_time(text):\n",
    "    text = re.sub(pattern=r\"kl \\d\\d . \\d\\d\", repl=\"[time]\", string=text, count=0)\n",
    "    return text\n",
    "\n",
    "def replace_time_position(text):    # for breast cancer, position is given with clock reference\n",
    "    text = re.sub(pattern=r\"kl \\. (\\d\\d?)\", repl=r\"kl \\1\", string=text, count=0)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the text before tokenizing further\n",
    "df[\"text\"] = df[\"text\"].apply(remove_prefix)\n",
    "df[\"text\"] = df[\"text\"].apply(replace_date)\n",
    "df[\"text\"] = df[\"text\"].apply(replace_decimal)\n",
    "df[\"text\"] = df[\"text\"].apply(replace_time)\n",
    "df[\"text\"] = df[\"text\"].apply(replace_time_position)\n",
    "df[\"text\"] = df[\"text\"].apply(replace_large_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize with nltk tokenizer\n",
    "# and undo nltk splits for replacement tokens\n",
    "def merge_replacement_tokens(tokenized_text, replacement_tokens=REPLACEMENT_TOKS):\n",
    "    # merge lists with dummy separator token\n",
    "    sep_token = \"<replacementtoken>\"\n",
    "    tmp_text = sep_token.join(tokenized_text)\n",
    "\n",
    "    # merge the replacement tokens back together\n",
    "    for repl_tok in replacement_tokens:\n",
    "        repl_tok_no_bracket = repl_tok[1:-1]    # [namn] > namn to insert it in replace function\n",
    "        \n",
    "        tmp_text = tmp_text.replace(f\"[{sep_token}{repl_tok_no_bracket}{sep_token}]\", repl_tok)\n",
    "    # delete the separator token again\n",
    "    cleaned_tokenized_text = tmp_text.split(sep_token)\n",
    "    return cleaned_tokenized_text\n",
    "\n",
    "\n",
    "def get_tokenized_text(text):\n",
    "    tokenized_text = word_tokenize(text, language=LANG)\n",
    "    clean_text = merge_replacement_tokens(tokenized_text)\n",
    "\n",
    "    assert type(clean_text) == list\n",
    "    return clean_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_senttokenized_text(text):\n",
    "    sent_tokenized_text = sent_tokenize(text, language=LANG)\n",
    "    cleaned_sents = []\n",
    "    for sent in sent_tokenized_text:\n",
    "        if len(cleaned_sents) == 0:     # just for first time\n",
    "            cleaned_sents.append(sent)\n",
    "            continue\n",
    "        if re.match(pattern=r\"(\\d+|i+|i+v|vi+) \\.$\", string=cleaned_sents[-1]): \n",
    "            # if the previous sentence ends with a bulletpoint-like enumeration \n",
    "            # (with latin or roman numbers) followed by fullstop \n",
    "            # -> add what comes after to this (previous) bullet point)\n",
    "            # otherwise, the sentence only consists of e.g. \"1 .\"\n",
    "            # which is unwanted, esp. for HiSAN models\n",
    "\n",
    "            cleaned_sents[-1] = cleaned_sents[-1] + \" \" + sent\n",
    "\n",
    "    \n",
    "        else:\n",
    "            cleaned_sents.append(sent)\n",
    "    # custom tokenize function is used here again\n",
    "    word_sent_tokenized_sents = [get_tokenized_text(sent) for sent in cleaned_sents]\n",
    "\n",
    "    return word_sent_tokenized_sents\n",
    "\n",
    "def get_longest_sent_per_text(wordsent_tokenized_text):\n",
    "    lengths = [len(sent) for sent in wordsent_tokenized_text]\n",
    "    return max(lengths)\n",
    "\n",
    "\n",
    "df[\"sent_tokenized_text\"] = df[\"text\"].apply(get_senttokenized_text)\n",
    "df[\"longest_sent\"] = df[\"sent_tokenized_text\"].apply(get_longest_sent_per_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_long_sents(senttokenized_text, max_len=50):\n",
    "    chunked_sents = []\n",
    "    for sentence in senttokenized_text:\n",
    "        while len(sentence) > max_len:\n",
    "            chunked_sents.append(sentence[:max_len])\n",
    "            sentence = sentence[max_len:]\n",
    "        if len(sentence) > 0:\n",
    "            chunked_sents.append(sentence)\n",
    "    assert max([len(sent) for sent in chunked_sents]) <= max_len\n",
    "    return chunked_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_sents(senttokenized_text):\n",
    "    return len(senttokenized_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_chunk_lengths(chunked_sents):\n",
    "    lengths = [len(chunked_sent) for chunked_sent in chunked_sents]\n",
    "    n_chunks = len(chunked_sents)\n",
    "\n",
    "    return round(sum(lengths) / n_chunks, ndigits=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"chunked_sent_tokenized_text\"] = df[\"sent_tokenized_text\"].apply(split_long_sents)\n",
    "df[\"n_chunked_sents\"] = df[\"chunked_sent_tokenized_text\"].apply(get_n_sents)\n",
    "df[\"avg_chunk_length\"] = df[\"chunked_sent_tokenized_text\"].apply(get_avg_chunk_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_tokens(tokenized_text):\n",
    "    return len(tokenized_text)\n",
    "\n",
    "def get_n_types(tokenized_text):\n",
    "    return len(set(tokenized_text))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_token(token, replacement_tokens=REPLACEMENT_TOKS):\n",
    "    if token.isalpha():\n",
    "        return True\n",
    "    elif token in replacement_tokens:\n",
    "        return True\n",
    "    elif token not in punctuation:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def get_n_alphab_tokens(tokenized_text):\n",
    "    return [is_valid_token(token) for token in tokenized_text].count(True)\n",
    "\n",
    "def get_n_alphab_types(tokenized_text):\n",
    "    return [is_valid_token(token) for token in set(tokenized_text)].count(True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"tokenized_text\"] = df[\"text\"].apply(get_tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## add more columns for text-related statisitcs\n",
    "\n",
    "df[\"n_sents\"] = df[\"sent_tokenized_text\"].apply(get_n_sents)\n",
    "\n",
    "df[\"n_tokens\"] = df[\"tokenized_text\"].apply(get_n_tokens)\n",
    "df[\"n_types\"] = df[\"tokenized_text\"].apply(get_n_types)\n",
    "\n",
    "\n",
    "df[\"n_alphab_tokens\"] = df[\"tokenized_text\"].apply(get_n_alphab_tokens)\n",
    "df[\"n_alphab_types\"] = df[\"tokenized_text\"].apply(get_n_alphab_types)\n",
    "\n",
    "df[\"avg_tokens_per_sent\"] = round(df[\"n_tokens\"] / df[\"n_sents\"], ndigits=3)\n",
    "df[\"avg_alphab_tokens_per_sent\"] = round(df[\"n_alphab_tokens\"] / df[\"n_sents\"], ndigits=3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old shape:  (33567, 30)\n",
      "New shape:  (33363, 30)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>orig_index</th>\n",
       "      <th>lab</th>\n",
       "      <th>remark</th>\n",
       "      <th>date</th>\n",
       "      <th>icdo3code</th>\n",
       "      <th>icdo3text</th>\n",
       "      <th>snomed3code</th>\n",
       "      <th>snomed3text</th>\n",
       "      <th>text</th>\n",
       "      <th>year</th>\n",
       "      <th>...</th>\n",
       "      <th>n_chunked_sents</th>\n",
       "      <th>avg_chunk_length</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>n_sents</th>\n",
       "      <th>n_tokens</th>\n",
       "      <th>n_types</th>\n",
       "      <th>n_alphab_tokens</th>\n",
       "      <th>n_alphab_types</th>\n",
       "      <th>avg_tokens_per_sent</th>\n",
       "      <th>avg_alphab_tokens_per_sent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [orig_index, lab, remark, date, icdo3code, icdo3text, snomed3code, snomed3text, text, year, site, subsite, site+subsite, histology, behavior, morphology, grade, sent_tokenized_text, longest_sent, chunked_sent_tokenized_text, n_chunked_sents, avg_chunk_length, tokenized_text, n_sents, n_tokens, n_types, n_alphab_tokens, n_alphab_types, avg_tokens_per_sent, avg_alphab_tokens_per_sent]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 30 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# there were about 200 texts that only consisted of \". . \", these are removed, too\n",
    "print(\"Old shape: \", df.shape)\n",
    "no_alphab_tokens = df[df[\"n_alphab_tokens\"] == 0][\"text\"].index\n",
    "df = df.drop(no_alphab_tokens,  inplace=False)\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "print(\"New shape: \", df.shape)\n",
    "df.tail(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7) Extract text duplicates: full and partial duplicates\n",
    "--> these will be removed from the dataset. Partial duplicates will still be saved, if needed for future projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2128, 30)\n"
     ]
    }
   ],
   "source": [
    "# choice to remove all text duplicates  so model only sees one code per identical text\n",
    "print(df[df.duplicated(subset=[\"text\"])].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicated texts:  2128\n"
     ]
    }
   ],
   "source": [
    "text_duplicate_indices = df[df.duplicated(\"text\")].index\n",
    "print(\"Number of duplicated texts: \", len(text_duplicate_indices))\n",
    "df[\"text\"][text_duplicate_indices].to_csv(os.path.join(clean_data_folder, \"tmp_inspect_dupliacted_texts.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old shape:  (33363, 30)\n",
      "New shape:  (31235, 30)\n"
     ]
    }
   ],
   "source": [
    "print(\"Old shape: \", df.shape)\n",
    "df_no_full_text_duplicates = df.drop_duplicates(\"text\", keep=\"first\", inplace=False, ignore_index=True)\n",
    "print(\"New shape: \", df_no_full_text_duplicates.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique texts: 29334\n",
      "Texts that are contained in other texts, too: 1901\n",
      "Full Duplicates: 0\n",
      "31235 == 31235\n"
     ]
    }
   ],
   "source": [
    "# Remove rows where there is another row with an additional remark, i.e.\n",
    "# text, text+remark, text+remark+remark; these texts are removed from the dataset\n",
    "\n",
    "assert list(df_no_full_text_duplicates.index) == list(range((df_no_full_text_duplicates.shape[0])))\n",
    "assert df_no_full_text_duplicates.index[0] == 0\n",
    "\n",
    "unique_texts = []\n",
    "partial_duplicate_texts = []\n",
    "duplicate_texts = []    \n",
    "# Note: duplicate_texts are not full duplicate rows, only the text is identical;\n",
    "# hence, there should be none at this point, they were removed before, so this is just a sanity check\n",
    "for i, ind in enumerate(df_no_full_text_duplicates.index[:-1]):                                     # -1 to prevent index error for last text\n",
    "    if i == 0: \n",
    "        text = df_no_full_text_duplicates[\"text\"][ind]\n",
    "        next_text = df_no_full_text_duplicates[\"text\"][df_no_full_text_duplicates.index[i+1]]\n",
    "        if text == next_text:\n",
    "            duplicate_texts.append(ind)\n",
    "        elif next_text.startswith(text) \\\n",
    "            and len(text) < len(next_text):    \n",
    "            # if the text is fully contained in the next text, then move the text to the duplicate list\n",
    "            partial_duplicate_texts.append(ind)\n",
    "        else:\n",
    "            unique_texts.append(ind)\n",
    "    else:\n",
    "        text = df_no_full_text_duplicates[\"text\"][ind]\n",
    "        next_text = df_no_full_text_duplicates[\"text\"][df_no_full_text_duplicates.index[i+1]]\n",
    "        previous_text = df_no_full_text_duplicates[\"text\"][df_no_full_text_duplicates.index[i-1]]\n",
    "        if text == next_text:\n",
    "            duplicate_texts.append(ind)\n",
    "        elif next_text.startswith(text) \\\n",
    "            and len(text) < len(next_text):    \n",
    "            # if the text is partially contained in the next text, then move the text to the half duplicate list\n",
    "            partial_duplicate_texts.append(ind)\n",
    "        \n",
    "        elif text.startswith(previous_text) \\\n",
    "            and len(previous_text) < len(text):\n",
    "            partial_duplicate_texts.append(ind)\n",
    "        else:\n",
    "            unique_texts.append(ind)\n",
    "\n",
    "if ind not in partial_duplicate_texts and ind not in duplicate_texts:\n",
    "    unique_texts.append(df_no_full_text_duplicates.index[-1])   # last text is always kept since duplicates would have been removed already\n",
    "\n",
    "partial_duplicate_texts = list(set(partial_duplicate_texts))    # remove indices that were added multiple times (if any)\n",
    "\n",
    "\n",
    "print(f\"Unique texts: {len(unique_texts)}\", f\"Texts that are contained in other texts, too: {len(partial_duplicate_texts)}\", f\"Full Duplicates: {len(duplicate_texts)}\", sep=\"\\n\")\n",
    "assert len(unique_texts) + len(partial_duplicate_texts) + len(duplicate_texts) == len(df_no_full_text_duplicates[\"text\"])\n",
    "print(len(unique_texts) + len(partial_duplicate_texts) + len(duplicate_texts), \"==\", len(df_no_full_text_duplicates[\"text\"]))\n",
    "\n",
    "df_no_full_text_duplicates[\"text\"][partial_duplicate_texts].to_csv(os.path.join(clean_data_folder, \"tmp_inspect_partial_dupliacted_texts.csv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29334, 30)\n",
      "(1901, 30)\n"
     ]
    }
   ],
   "source": [
    "df_no_text_duplicates = df_no_full_text_duplicates.loc[unique_texts]\n",
    "assert len(df_no_text_duplicates[\"text\"]) == len(set(df_no_text_duplicates[\"text\"]))    # only unique texts\n",
    "\n",
    "\n",
    "df_partial_duplicates = df_no_full_text_duplicates.loc[partial_duplicate_texts]\n",
    "\n",
    "\n",
    "print(df_no_text_duplicates.shape)\n",
    "print(df_partial_duplicates.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8) Save dataframes\n",
    "\n",
    "as pkl-files to keep their datatypes as they are; esp. important for the lists of tokenized text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save final dataframes\n",
    "\n",
    "# for computing (since data types are preserved in pkl file)\n",
    "df_no_text_duplicates.to_pickle(f\"{data_cleaned}.pkl\")\n",
    "df_partial_duplicates.to_pickle(f\"{data_partial_duplicates}.pkl\")\n",
    "\n",
    "# for easier manual inspection\n",
    "df_no_text_duplicates.to_csv(f\"{data_cleaned}.csv\", encoding=\"utf-8\")\n",
    "df_partial_duplicates.to_csv(f\"{data_partial_duplicates}.csv\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9) Investigate Vocabulary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_text_duplicates_voc = Vocabulary(df_no_text_duplicates[\"tokenized_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39579"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_no_text_duplicates_voc.vocabsize(min_count=1)\n",
    "# Note: For model training, words with low frequencies are removed, \n",
    "# so vocabulary size will be smaller in model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "frq_counts_df_no_text_duplicates = pd.DataFrame.from_dict(dict(df_no_text_duplicates_voc.word2frq), \n",
    "                                                          orient=\"index\", columns=[\"frq\"])\n",
    "frq_counts_df_no_text_duplicates = frq_counts_df_no_text_duplicates.sort_values(\"frq\", ascending=False)\n",
    "\n",
    "frq_counts_df_no_text_duplicates.to_csv(os.path.join(clean_data_folder, \n",
    "                                                     \"vocabulary_data_clean_no_dup.csv\"), encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_no_text_duplicates_voc.numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_no_text_duplicates_voc.single_letter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_no_text_duplicates_voc.single_letter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_no_text_duplicates_voc.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    29334.000000\n",
       "mean        33.163394\n",
       "std         32.867342\n",
       "min          1.000000\n",
       "25%         18.000000\n",
       "50%         25.000000\n",
       "75%         36.000000\n",
       "max        745.000000\n",
       "Name: longest_sent, dtype: float64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_no_text_duplicates[\"longest_sent\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(For data protection, the files below are not made available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_text_duplicates[\"tokenized_text\"].to_csv(os.path.join(clean_data_folder,\"tmp_maunally_inspect_tokenized_text.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_text_duplicates[\"text\"].to_csv(os.path.join(clean_data_folder,\"tmp_maunally_inspect_text.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_text_duplicates[\"sent_tokenized_text\"].to_csv(os.path.join(clean_data_folder,\"tmp_maunally_inspect_sent_tokenized_text.csv\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
